# <a name="_1mv6g0oc6axa"></a>Learning Contract Innovation
Share\
COURSE: ENGL 1101 career-technical writing | 24 students (14 regular, 10 remedial) | Twice-weekly 75min classes + 25min remedial | No homework, all work in-class | Instructor: department chair with admin duties CORE PHILOSOPHY: - Process and growth over final product - Individualized grading ba\
pasted\
**C**\
I think I have a breakthrough on how to handle the question of day-to-day activity...\
Edit\
I'm excited to hear about your breakthrough! The learning contract framework you've developed is really sophisticated - especially the chip allocation system that makes resource management tangible and the dual tracking views that serve both strategic growth and assignment progress.\
What's your insight about handling the day-to-day activities? I can see from your document that this was identified as one of the key unresolved implementation questions, particularly around creating specific menu options for each growth area and connecting daily focus to assignment progress.\
I'm curious how your breakthrough addresses the balance between giving students meaningful choice in their daily activities while ensuring those activities connect authentically to their career preparation and current writing assignments.\
Retry\
**C**\
The real question comes back to the diagnostic - I ask students to rate their level of confidence and competency relevant to the outcomes, but there is an extra question for each of these that hasn't been brought up. Why? If you think you are below standard, why? This gets at the core of the deficiency. Is it cognitive (I don't know this...), applicative (I can't do this..), or affective (I don't like/feel confident). If the student is asked in their own (PRIVATE) assessment to identify where they think the problem is, that gives us a perfect directive where we can create suggested pathways for every outcome and sub-outcome, with learning activity recommendations based on each of the three domains that the student can choose from, or select their own, with the idea that will become a topic of conversation. If they choose Other on anything, the tick box is toggled orange, as a note for future conversation at the next conference.\
Edit\
That's brilliant! You've identified the missing diagnostic layer that makes the whole system work. By having students privately identify whether their perceived deficiency is cognitive, applicative, or affective, you're essentially creating a branching pathway system that can generate personalized activity menus automatically.\
So for each growth area and sub-outcome, you'd have three different types of recommended activities:\
Cognitive deficit → Knowledge-building activities (research, reading, concept mapping, AI-assisted explanations)\
Applicative deficit → Practice-based activities (skill drills, guided practice, peer collaboration, repeated application)\
Affective deficit → Confidence-building activities (low-stakes practice, success tracking, peer mentoring, reflection on past successes)\
This solves multiple problems at once:

1. Personalizes the daily menu - Students see activities tailored to their actual barrier type
1. Reduces decision paralysis - Instead of overwhelming choice, they get 3-4 targeted options plus "Other"
1. Creates natural conference topics - Orange flags for "Other" choices guide your 7-minute discussions
1. Maintains student agency - They still choose, but with intelligent scaffolding
1. Addresses the equity concern - Different students with the same outcome goal get different pathways based on their actual needs

The private nature of this assessment is crucial too - students can be honest about affective barriers without public vulnerability. How are you thinking about structuring this diagnostically? A simple dropdown for each outcome where they select their confidence level AND their perceived barrier type?\
Retry\
**C**\
If I could find some kind of assessment or input tool that allows for sliding scales for the outcomes and sub-outcomes, then a drop down for each question, that would be ideal, even if it comes from outside of Google (our institutional ecosystem...) We use Canvas, so if there is a quiz setting that could allow this. Any recommendations would be most welcome.\
Edit\
Canvas quiz slider scale question types 2024\
10 results\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
Canvas New Quizzes slider scale question types\
10 results\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
Based on my search, here are your options for creating sliding scale assessments:
## <a name="_bfz395qhghf7"></a>**Canvas Options:**
Canvas Classic Quizzes: Canvas' Multiple Dropdown question type can be used to create Likert scale questions [Instructure Community](https://community.canvaslms.com/t5/Instructor-Guide/How-do-I-create-Likert-scale-quiz-questions/ta-p/942)\
[CTLD Ready](https://ready.msudenver.edu/self-help-tutorials/new-quizzes/create-different-types-of-new-quiz-questions/)\
, but this doesn't provide true slider functionality - it's dropdown selections.\
Canvas New Quizzes: Canvas plans to permanently switch to New Quizzes by the end of June 2024 [Canvas New Quizzes - Canvas Resources](https://canvas.charlotte.edu/canvas-new-quizzes/)\
and New Quizzes has a number of new question types, along with additional security features and reporting tools [Cal Poly Humboldt](https://www.humboldt.edu/canvas-lms/new-quizzes-set-and-question-types)\
[Fullerton](https://canvashelp.fullerton.edu/m/Making_the_Most_of_Canvas/l/1444498-how-do-i-create-different-types-of-questions-for-a-quiz)\
. However, my search didn't reveal specific slider scale functionality in New Quizzes either.
## <a name="_dzkz2wms27tn"></a>**Alternative Recommendations:**
1. Google Forms (integrates well with your existing Google ecosystem): Has built-in linear scale questions (1-10 scales) with customizable labels, plus dropdown options for the "why" questions. Data exports easily to Google Sheets for analysis.
1. Microsoft Forms (if your institution has Office 365): Offers Likert scale questions and can export to Excel.
1. Qualtrics (if available through your institution): Has robust slider question functionality [Slider Question](https://www.qualtrics.com/support/survey-platform/survey-module/editing-questions/question-types-guide/standard-content/slider/)
1. and sophisticated branching logic.
1. Canvas Survey Tool: Many institutions have survey tools that integrate with Canvas through LTI.

My Recommendation: Given your Google ecosystem preference, Google Forms might be your best bet. You can create:

- Linear scale questions (1-4) for confidence/competency ratings
- Dropdown questions for the cognitive/applicative/affective categories
- Automatic data collection in Google Sheets
- Easy sharing via Canvas links

Would you like me to help you structure the specific question format for Google Forms, or would you prefer to explore what your institution might have available through Canvas LTI integrations?\
Retry\
**C**\
Does Canvas allow for multiple dropdowns within a single question?\
Edit\
Canvas multiple dropdown single question quiz\
10 results\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
favicon\
"Canvas multiple dropdown" question example\
0 results\
Yes! Canvas does have a Multiple Dropdown question type that allows you to create exactly what you need. You can create a question that has multiple options for students to select in their response. You can create a sentence with multiple answers, as demonstrated in this lesson, or you can use the same steps to create a list. [Slider Question | Slider scale survey | QuestionPro](https://www.questionpro.com/features/sliderscale.html)\
Here's how it would work for your diagnostic:\
Example Question Structure:

For [Outcome/Sub-outcome]: "Audience Analysis"

My confidence level is [Dropdown 1: 1-Not confident, 2-Somewhat confident, 3-Confident, 4-Very confident]

My competency level is [Dropdown 2: 1-Below standard, 2-Approaching standard, 3-At standard, 4-Above standard]  

If below standard, this is because [Dropdown 3: Cognitive-I don't know this, Applicative-I can't do this, Affective-I don't feel confident, N/A-I'm at/above standard]



Important Note: Multiple dropdown questions will display as Matching questions in New Quizzes [What question types are available in Canvas? - Office of Digital Learning](https://support.canvas.fsu.edu/kb/article/822-what-question-types-are-available-in-canvas/)\
, so if your institution has switched to New Quizzes, the functionality changes. However, New Quizzes does have "Single/Multiple Dropdown" questions in the Fill in the Blank category [Canvas: How to Create Different Types of Quiz Question – Instruction @ UH](https://www.instruction.uh.edu/knowledgebase/how-to-create-different-types-of-quiz-question/)\
.\
This Canvas solution would be perfect because:

- Keeps everything in your institutional ecosystem
- Allows multiple dropdowns per question (one for each outcome)
- Automatically collects data in Canvas gradebook
- Students can save and return to complete
- You can export data for analysis

Would you like me to help you structure the specific question format for each of your learning outcomes using this Canvas Multiple Dropdown approach?\
Retry\
Claude does not have the ability to run the code it generates yet.\
**C**\
I am not sure if we need to ask for both confidence and competency; they seem interrelated. And what am I going to do with students affected with Dunning-Kreuger or who are perfectly confident in very low competency levels?\
Edit\
You're absolutely right - that's a crucial insight. Confidence and competency often don't align, especially in writing where students may be confident in practices that are actually counterproductive, or lack confidence in areas where they're actually quite skilled.\
The Dunning-Kruger problem is real: students who are most deficient often rate themselves as most competent, while skilled students underestimate their abilities. This makes self-reported "competency" nearly meaningless.\
Better approach: Focus on perceived barriers rather than competency ratings. Instead of asking students to judge their skill level (which they can't accurately assess), ask them to identify what they think is getting in their way.\
Revised diagnostic structure:

For [Outcome]: "Audience Analysis"

When I think about analyzing my audience for workplace writing, my biggest barrier is:

[Dropdown: 

\- I don't understand what audience analysis means (Cognitive)

\- I understand it but struggle to actually do it (Applicative)  

\- I understand it but feel anxious/overwhelmed when I try (Affective)

\- I don't see this as a barrier for me (Skip)

\- Other (explain)]



This reframing:

- Sidesteps the competency assessment trap
- Focuses on actionable barriers
- Reduces defensiveness (barrier vs. deficiency)
- Still generates the same three pathway types
- Gives you more honest, useful data

The "Other" option with orange flagging becomes even more valuable - it captures students who have unique barriers or who might be overconfident about areas where they actually need work.\
What do you think about this barrier-focused approach?\
Retry\
Claude does not have the ability to run the code it generates yet.\
**C**\
I like this; it moves away from the potential self-recrimination of "Man, I marked far below on everything, I am so stupid...." to "OK, these are areas I need to work on and I don't have to focus on the stuff I feel I am good at..."\
Edit\
Exactly! You've hit on something really important about the psychology of self-assessment. The barrier-focused approach is inherently growth-oriented rather than deficit-focused.\
When students identify barriers instead of rating competency, they're essentially saying "Here's what I need to work through" rather than "Here's how bad I am." It shifts the entire mindset from:\
Deficit thinking: "I'm a 1 out of 4 in everything = I'm terrible" Growth thinking: "I have specific barriers I can address = I have a plan"\
This also solves the strategic allocation problem beautifully. Instead of students thinking they need to spread their chips across everything they're "bad at," they can focus their limited resources on the specific barriers that are actually blocking their progress.\
The psychological safety aspect is huge too - students can honestly identify affective barriers without feeling like they're admitting incompetence. "I feel anxious about audience analysis" is very different from "I'm incompetent at audience analysis."\
Plus, this approach naturally leads to more productive conference conversations. Instead of "Why did you rate yourself so low?" you get "Tell me more about what makes audience analysis feel overwhelming" - much more actionable and supportive.\
The barrier identification also makes the activity menu generation much more intuitive. Students aren't just getting "beginner activities" - they're getting activities specifically designed to address their actual roadblock, whether that's knowledge, application, or confidence.\
This is a really elegant solution that maintains the diagnostic power while being much more humane and motivating.\
Retry\
**C**\
And this has them only answering one question, but giving us multiple points of data at a time...\
Edit\
Yes! That's the elegant efficiency of it. One question per outcome, but you get:\
Multiple data points:

- Which outcomes they see as barriers (resource allocation guidance)
- What TYPE of barrier for each (pathway selection)
- Which outcomes they feel confident about (no chip allocation needed)
- Unique situations via "Other" (conference priorities)

Single interaction instead of the original 2-3 questions per outcome. For 6 growth areas with multiple sub-outcomes, you just saved students from answering potentially 36+ questions down to maybe 12-15.\
Cleaner data too - instead of trying to correlate confidence ratings with competency ratings with barrier types, you get one clear, actionable response per outcome that directly maps to:

- Chip allocation decisions
- Activity menu generation
- Conference talking points
- Progress tracking categories

The cognitive load reduction is significant for both students AND you. Students aren't decision-fatigued by the assessment process, and you're not trying to interpret complex multi-variable data to figure out what each student actually needs.\
This streamlined approach also makes the whole system more sustainable - easier to set up, easier for students to complete honestly, and easier for you to use the data meaningfully in your 7-minute conferences.\
Really nice problem-solving here - you've found a way to get richer, more actionable data through simplification rather than complexity.\
Retry\
**C**\
OK. Let me draft something in Canvas, print the quiz as a pdf and have you take a look at it.\
Edit\
Perfect! That's a great way to test the actual implementation. When you create the Canvas quiz and print it as a PDF, I'll be able to see exactly how the Multiple Dropdown questions format and whether the flow feels natural for students.\
I'll be looking for:

- Clarity of the barrier language
- Logical flow between outcomes and sub-outcomes
- Whether the dropdown options feel comprehensive but not overwhelming
- How the "Other" option integrates
- Overall cognitive load for students

Take your time with the draft - this is the kind of thing that benefits from seeing it in the actual platform format rather than just conceptually. The Canvas interface will reveal any formatting or user experience issues that might not be obvious when we're just talking through the concept.\
Looking forward to seeing how it translates into practice!\
Retry\
**C**\
Actually, based on what you just said, I don't need multiple dropdowns, just one per specific outcome.\
Edit\
Exactly! You're absolutely right. Each outcome gets one dropdown question with the barrier options:

When I think about [specific outcome], my biggest barrier is:

\- I don't understand what this means (Cognitive)

\- I understand it but struggle to actually do it (Applicative)  

\- I understand it but feel anxious/overwhelmed when I try (Affective)

\- I don't see this as a barrier for me (Skip)

\- Other (explain)



This is much cleaner than Multiple Dropdown questions. Each outcome becomes its own simple dropdown question, which means:

- Easier to build in Canvas (standard dropdown, not the more complex Multiple Dropdown)
- Easier for students to navigate (one decision per screen/question)
- Cleaner data export (one column per outcome instead of trying to parse multiple dropdowns)
- More intuitive flow for students moving through the assessment

Plus, if you organize the quiz with one outcome per page/question, students can see their progress through the diagnostic more clearly, and you can potentially add brief explanations of what each outcome means right above each dropdown if needed.\
Much more straightforward implementation. Good catch!\
Retry\
Claude does not have the ability to run the code it generates yet.\
**C**\
The quiz exported as an xml document, can you process information in that format?\
Edit\
I can absolutely work with XML! XML is a structured markup language that I can read and analyze. You can either:

1. Upload the XML file directly - I can read uploaded files and parse the XML structure to see your quiz questions, dropdown options, and overall flow
1. Copy and paste the XML content - If you open the XML file in a text editor and copy the content, I can analyze it that way too

XML will actually give me a very detailed view of how Canvas structured your quiz - I'll be able to see the question types, the exact dropdown options, any explanatory text, and how the questions are ordered. This might be even better than a PDF for understanding the technical implementation.\
Go ahead and upload the XML file when you're ready, and I'll take a look at how your barrier-focused diagnostic translated into the actual Canvas quiz format!\
Retry\
**C**\
Can you update the markdown document to reflect the conversation thus far?\
Edit\
Untitled\
Text\
\## RESOLVED IMPLEMENTATION BREAKTHROUGH: BARRIER-FOCUSED DIAGNOSTIC

\### Day-to-Day Activity Structure Solution:\
The diagnostic assessment includes a critical third layer: \*\*barrier identification\*\*. For each outcome/sub-outcome where students identi\
The document has been updated to reflect your breakthrough on the barrier-focused diagnostic approach. The key insight about having students identify the type of barrier (cognitive, applicative, or affective) rather than rating their competency levels is now captured in the resolved implementation section, along with how this creates automatic personalized activity pathways and streamlines the entire assessment process.\
Retry\
**C**\
But how does this connect with the broader conversation document i uploaded at the beginning of the conversation?\
Edit\
You're absolutely right - I need to integrate this breakthrough into the broader framework you've developed. Let me update the document to show how the barrier-focused diagnostic connects with your chip allocation system, learning contracts, and overall course structure.\
Untitled\
Text ∙ Version 2\
\### Contract Development Process:\
1\. \*\*Barrier-focused diagnostic\*\* - Students identify specific barriers (cognitive, applicative, affective) for each outcome rather than rating competency levels\
2\. \*\*Offline personal assessment\*\* - Private reflectio\
Untitled\
Text ∙ Version 3\
\### Daily Activity Implementation:\
Students select specific activities within their allocated focus areas based on \*\*barrier-type pathways\*\*:\
\- \*\*Cognitive barriers\*\*: Knowledge-building activities (research, concept mapping, AI explanations)\
\- \*\*App\
Untitled\
Text ∙ Version 4\
\### Addressing Core Tensions:\
\- \*\*AI Integration\*\*: Tool for thinking enhancement, not content generation; provides barrier-specific activity feedback\
\- \*\*Equity Focus\*\*: Barrier-focused assessment eliminates Dunning-Kruger effects and reduces self-r\
Untitled\
Text ∙ Version 5\
\### Pedagogical Innovations:\
\- \*\*Barrier-focused diagnostic\*\*: Eliminates competency self-assessment problems while generating personalized pathways\
\- \*\*Chip allocation\*\*: Makes abstract priority-setting concrete and gamified, applied only to identif\
Untitled\
Text ∙ Version 6\
\### Week 1-2: Contract Development\
\- Administer barrier-focused diagnostic using Canvas dropdown questions\
\- Facilitate offline personal reflection on career goals and barrier implications\
\- Guide strategic chip allocation process for identified barr\
Now the breakthrough is properly integrated throughout the broader framework. The barrier-focused diagnostic becomes the foundation that drives the chip allocation strategy, shapes the daily activity pathways, addresses the core pedagogical tensions, and connects to your conference structure and AI integration approach. The system now has a clear progression from barrier identification → strategic resource allocation → personalized pathway selection → targeted skill development.\
Retry\
[Claude can make mistakes. Please double-check responses.](https://support.anthropic.com/en/articles/8525154-claude-is-providing-incorrect-or-misleading-responses-what-s-going-on)\
Sonnet 4\
Claude

**C**\
Clayn\
Free plan

